{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQtpx-EPaEXs"
      },
      "source": [
        "# EEP 467 Homework 2: Breaking CAPTCHAs End-to-End\n",
        "\n",
        "In lab 2, we have successfully built a pipeline that automatically recognizes CAPTCHAs. So far, the recognition process requires manually identifying contours (and subsequently bounding boxes) around characters using expert rules, before performing classification on extracted character images. This inspires us for possible further improvement: can we use a single neural network to directly identify CAPTCHA characters from CAPTCHA image? In this homework, we will get rid of bounding box identification and build an end-to-end neural network model.\n",
        "\n",
        "## Before You Start\n",
        "\n",
        "1. You can discuss on Discord if you have questions and want to seek help; however, please try your best to **limit the scope of your question** and **avoid asking directly for answers**. You should also **avoid copy-pasting answers and code** from others.\n",
        "2. You are allowed to use AI assistants for help. In this case, you should **acknowledge your use of AI assistant** by providing its **name as well as how it helps you** to obtain your answer. Again, you **should not copy-paste from the response** of the AI assistant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPw6-VYvaEXv"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "First of all, make sure you have installed all dependencies needed for this homework. The list of depedencies is similar to lab 3, except that we will ditch OpenCV and `imutils`, and instead use PyTorch Vision (`torchvision`) for some of our preprocessing work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtJn8UyFaEXv"
      },
      "outputs": [],
      "source": [
        "%pip install matplotlib scikit-learn tqdm\n",
        "\n",
        "# If using Anaconda / Miniconda, install PyTorch and PyTorch Vision with:\n",
        "# %conda install conda-forge::pytorch-gpu conda-forge::torchvision # (CPU and GPU support)\n",
        "# %conda install conda-forge::pytorch conda-forge::torchvision # (CPU-only support)\n",
        "\n",
        "# If using Pip, install PyTorch and PyTorch Vision with:\n",
        "%pip install torch torchvision\n",
        "# When installing PyTorch with GPU support, refer to https://pytorch.org/get-started/locally/\n",
        "# if your CUDA version differs from the default CUDA version of the current version PyTorch package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow1P6GvJaEXw"
      },
      "source": [
        "Once you are ready, we will start with data pre-processing step where we load all CAPTCHA images into memory. There are a few utilities in `torchvision` which can help us do the job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nggjXxBaEXw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
        "from torchvision.io import ImageReadMode, decode_image\n",
        "\n",
        "# CAPTCHA images directory\n",
        "CAPTCHA_IMAGES_DIR = \"captcha-images\"\n",
        "\n",
        "# CAPTCHA images\n",
        "captcha_images = []\n",
        "# CAPTCHA texts\n",
        "captcha_texts = []\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Load all CAPTCHA images from `CAPTCHA_IMAGES_DIR` into `captcha_images`\n",
        "#    (Hint: use `os.scandir` to iterate over directory entries and check file extensions with `IMG_EXTENSIONS`)\n",
        "#    (Hint: use `torchvision.io.decode_image` to load images in grayscale mode)\n",
        "#    (Note: your code should ignore non-image files)\n",
        "# 2) Stack all CAPTCHA images into a PyTorch tensor\n",
        "# 3) Load all CAPTCHA texts (filenames without extension) into `captcha_texts`\n",
        "#    (Items in `captcha_images` and `captcha_texts` should match)\n",
        "pass\n",
        "\n",
        "assert isinstance(captcha_images, torch.Tensor), \"`captcha_images` must be a PyTorch tensor!\"\n",
        "assert captcha_images.ndim==4, \"`captcha_images` should be a 4D tensor!\"\n",
        "assert captcha_images.shape[1:]==(1, 24, 72), \"`captcha_images` should have shape (n_images, 1, h, w)!\"\n",
        "\n",
        "assert len(captcha_images)==len(captcha_texts), \\\n",
        "    \"`captcha_images` should have the same number of elements as `captcha_texts`!\"\n",
        "assert all(isinstance(text, str) and len(text)==4 for text in captcha_texts), \\\n",
        "    \"all `captcha_texts` should be strings with four letters!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60yi0QsSaEXx"
      },
      "source": [
        "Then, we split all CAPTCHA images and texts into training-validation and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi32VXbzaEXx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Training-validation-test split seed\n",
        "TVT_SPLIT_SEED = 31528476\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "captcha_images_tv, captcha_images_test, captcha_texts_tv, captcha_texts_test = train_test_split(\n",
        "    captcha_images, captcha_texts, test_size=0.2, random_state=TVT_SPLIT_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL5yVwZkaEXx"
      },
      "source": [
        "Next, we encode each character in the CAPTCHA text as class indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lnwvEK9aEXx"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import hw2_util\n",
        "\n",
        "# Label encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Flatten training-validation CAPTCHA texts into list of characters\n",
        "#    (Hint: `hw2_util.flatten` can help you with that)\n",
        "# 2) Fit and transform characters into class indices using `LabelEncoder`\n",
        "# 3) Save number of classes (distinct characters) in `n_classes`\n",
        "# 4) Convert class indices to PyTorch tensor and reshape as `texts_class_indices_tv`\n",
        "#    (The shape of `texts_class_indices_tv` should be (n_images, 4))\n",
        "pass\n",
        "\n",
        "assert isinstance(texts_class_indices_tv, torch.Tensor), \"`texts_class_indices_tv` must be a PyTorch tensor!\"\n",
        "assert texts_class_indices_tv.ndim==2, \"`texts_class_indices_tv` must have two dimensions!\"\n",
        "assert texts_class_indices_tv.shape==(len(captcha_texts_tv), 4), \\\n",
        "    \"Shape of `texts_class_indices_tv` should be (n_images, 4)!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFjedP7naEXy"
      },
      "source": [
        "Then, we further split training-validation set into two parts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9qBmStgaEXy"
      },
      "outputs": [],
      "source": [
        "# Split training set further into training and validation sets\n",
        "captcha_images_train, captcha_images_vali, texts_class_indices_train, texts_class_indices_vali = train_test_split(\n",
        "    captcha_images_tv, texts_class_indices_tv, test_size=0.25, random_state=TVT_SPLIT_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZoUEYPgaEXy"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "The following step will be unique to this homework: we will perform **data augmentation** using PyTorch's [`transforms.Compose`](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.Compose.html) and [`transforms.RandomAffine`](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandomAffine.html). Here we will configure the `RandomAffine` transform to **apply the following random effects** to our CAPTCHAs:\n",
        "\n",
        "* Random rotation (at most 5 degrees)\n",
        "* Random horizontal and vertical shifting (at most 10%)\n",
        "* Random shearing (at most 5 degrees)\n",
        "* Random zooming in or out (at most 10%)\n",
        "\n",
        "Besides, we will also **pre-process the image pixels** in the `RescaleInvertPixels` custom transform. Here we need to first **rescale the grayscale values** to $[0, 1]$ range and then **invert the grayscales**. We will chain the `RescaleInvertPixels` transform together with the `RandomAffine` transform using `Compose` to create a complete augmentation pipeline.\n",
        "\n",
        "Below code will create such a transform pipeline, and preview its effects on a few sample CAPTCHAs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV6lILpeaEXy"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torchvision.transforms import Compose, InterpolationMode, RandomAffine\n",
        "\n",
        "class RescaleInvertPixels(nn.Module):\n",
        "    def forward(self, raw_image):\n",
        "        ## [ TODO ]\n",
        "        # 1) Rescale pixel grayscale values to [0, 1]\n",
        "        # 2) Invert the grayscales of the image (darker becomes lighter, and vice versa)\n",
        "        # 3) Return modified grayscales\n",
        "        raise NotImplementedError\n",
        "\n",
        "## [ TODO ]\n",
        "# Create a transform pipeline using `Compose` with:\n",
        "# 1) `RescaleInvertPixels()` for preprocessing\n",
        "# 2) `RandomAffine` with the following random effects:\n",
        "#    - Rotation: at most 5 degrees\n",
        "#    - Horizontal and vertical shift: at most 10%\n",
        "#    - Zooming: at most 10% (scale between 0.9 and 1.1)\n",
        "#    - Shearing: at most 5 degrees\n",
        "#    - Use `InterpolationMode.BILINEAR` for interpolation\n",
        "#\n",
        "# Choose parameters such that CAPTCHAs are moderately distorted but still recognizable.\n",
        "captcha_transform = NotImplemented\n",
        "\n",
        "# Select a few images for preview\n",
        "preview_orig = captcha_images_train[:5]\n",
        "# Generate a few transformed CAPTCHA images for preview\n",
        "preview_trans = captcha_transform(preview_orig)\n",
        "\n",
        "# Preview original and transformed CAPTCHAs\n",
        "hw2_util.print_images(\n",
        "    preview_orig.squeeze(-3),\n",
        "    texts=[f\"Raw: {i+1}\" for i in range(5)]\n",
        ")\n",
        "hw2_util.print_images(\n",
        "    preview_trans.squeeze(-3),\n",
        "    texts=[f\"Augumented: {i+1}\" for i in range(5)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw759NWqaEXy"
      },
      "source": [
        "Apart from the inversion of grayscale, you should be able to observe slight differences between original and transformed images. These transformed CAPTCHAs should still be easily recognizable, though.\n",
        "\n",
        "## Training\n",
        "\n",
        "From now on, we will shift our focus to the end-to-end neural network model. We assume our neural network model takes a bunch of CAPTCHA images (known as a batch) as input and outputs logits for each of the four character positions. In other words, if we have `n_classes` distinct characters in all CAPTCHAs, the output of our model would be a PyTorch tensor of shape `(batch_size, 4, n_classes)`. From each position's logits, we choose the character with the highest logit value (using the `argmax` operator), and we define a correct prediction as the one where all four characters are correctly predicted.\n",
        "\n",
        "Our first task is to implement custom loss and accuracy functions for our task, since PyTorch's built-in functions expect different output formats. Both `pred_logits` (Raw model outputs) and `actual_class_indices` (Ground truth class indices) are batch tensors of shape `(batch_size, 4, n_classes)` and `(batch_size, 4)` respectively. Here we use PyTorch operations to compute our custom loss and accuracy metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8JCe0e0aEXy"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as f\n",
        "\n",
        "def multi_char_loss(pred_logits, actual_class_indices):\n",
        "    \"\"\" Compute the mean cross entropy loss of multiple CAPTCHA characters recognition. \"\"\"\n",
        "    return f.cross_entropy(pred_logits.flatten(0, 1), actual_class_indices.flatten())\n",
        "\n",
        "def multi_char_acc(pred_logits, actual_class_indices):\n",
        "    \"\"\" Compute the accuracy of multiple CAPTCHA characters recognition. \"\"\"\n",
        "    # Compute predicted class indices (most likely class / largest logits)\n",
        "    pred_class_indices = pred_logits.argmax(-1)\n",
        "    # Compute prediction accuracy for ALL four characters in a CAPTCHA, then average across samples\n",
        "    return (pred_class_indices==actual_class_indices).all(-1).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTbay06zaEXy"
      },
      "source": [
        "Next, we will complete `build_model` function which is responsible for building the neural network model. The neural network starts with several \"convolution blocks\" as usual, each of which contains a convolution layer for feature extraction and a max pooling layer for dimensionality reduction. The latter part of the network is however different from lab 3 in that **we replace regular fully-connected layers (implemented with linear layers) with \"zone-wise\" fully-connected layers (implemented with convolution layers)**.\n",
        "\n",
        "We also substitute the regular cross-entropy loss for our custom `multi_char_loss` and make use of our custom accuracy metric, `multi_char_acc`, due to the unique output format of our model.\n",
        "\n",
        "Below is the structure of our network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whDOo0U8aEXy"
      },
      "outputs": [],
      "source": [
        "from hw2_util import Transpose\n",
        "\n",
        "def build_model(n_classes):\n",
        "    \"\"\" Build the neural network model for CAPTCHA classification. \"\"\"\n",
        "    return nn.Sequential(\n",
        "        # [ TODO ]\n",
        "        # Complete the model architecture according to the specifications shown above and below:\n",
        "        #\n",
        "        # First convolution block: (*, 1, 24, 72) -Conv2d-> (*, 10, 24, 72) -ReLU-> (*, 10, 24, 72) -MaxPool2d-> (*, 10, 12, 36)\n",
        "        # 1) Convolution layer: 1 input channel, 10 output channels, 5*5 kernel, padded to maintain same shape\n",
        "        # 2) ReLU activation layer\n",
        "        # 3) Max pooling layer: 2*2 kernel\n",
        "        NotImplemented,\n",
        "        # Second convolution block: (*, 10, 12, 36) -Conv2d-> (*, 40, 12, 36) -ReLU-> (*, 40, 12, 36) -MaxPool2d-> (*, 40, 6, 18)\n",
        "        NotImplemented,\n",
        "        # Third convolution block: (*, 40, 6, 18) -Conv2d-> (*, 100, 6, 18) -ReLU-> (*, 100, 6, 18) -MaxPool2d-> (*, 100, 3, 9)\n",
        "        NotImplemented,\n",
        "        # First \"zone-wise\" fully-connected block: (*, 100, 3, 9) -Conv2d-> (*, 800, 1, 4) -ReLU-> (*, 800, 1, 4)\n",
        "        # Convolution layer: 100 input channels, 800 output channels, 3*3 kernel, horizontal stride set to 2 to obtain 4 zones, no padding\n",
        "        NotImplemented,\n",
        "        # Second \"zone-wise\" fully-connected layer: (*, 800, 1, 4) -Conv2d-> (*, n_classes, 1, 4)\n",
        "        # Convolution layer: `n_classes` channels, 1*1 kernel, no padding, used for dimensionality reduction\n",
        "        NotImplemented,\n",
        "        # Reshape and transpose dimensions: (*, n_classes, 1, 4) -Flatten-> (*, n_classes, 4) -`lab_3_helpers.Transpose`-> (*, 4, n_classes)\n",
        "        NotImplemented\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM36_laVaEXz"
      },
      "source": [
        "To facilitate efficient data loading and transformation, we will create a custom PyTorch [`Dataset`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class called `CAPTCHADataset`. This class will handle loading CAPTCHA images and their corresponding labels, and apply the data augmentation transforms on-the-fly during training. By implementing the standard PyTorch [`Dataset`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) interface, we can leverage PyTorch's [`DataLoader`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) for efficient batch processing and parallel data loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHwbaseXaEXz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CAPTCHADataset(Dataset):\n",
        "    def __init__(self, captcha_images, texts_features, captcha_transform):\n",
        "        self.captcha_images = captcha_images\n",
        "        self.texts_features = texts_features\n",
        "        self.captcha_transform = captcha_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captcha_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        captcha_image = self.captcha_transform(self.captcha_images[idx])\n",
        "        text_features = self.texts_features[idx]\n",
        "\n",
        "        return captcha_image, text_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylp9rt1TaEXz"
      },
      "source": [
        "Finally, the `train_model` function implements the standard PyTorch training loop. For each epoch, it performs forward passes on batches from the training set, computes loss using `multi_char_loss`, backpropagates gradients, and updates model parameters. After each epoch, it evaluates the model on the validation set. Data augmentation is applied on-the-fly via the `CAPTCHADataset` during both training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm5KhvetaEXz"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_model(model, optimizer, dataset_train, dataset_vali, batch_size=32, n_epochs=50, device=None):\n",
        "    # Detect device from model parameters if not provided\n",
        "    device = device or next(iter(model.parameters())).device\n",
        "\n",
        "    ## [ TODO ]\n",
        "    # 1) Create training data loader from `dataset_train` with shuffling enabled\n",
        "    # 2) Create validation data loader from `dataset_vali`\n",
        "    # (Note: set the batch size for both loaders)\n",
        "    loader_train = NotImplemented\n",
        "    loader_vali = NotImplemented\n",
        "\n",
        "    history = []\n",
        "\n",
        "    print(\"Starting training ...\")\n",
        "    for i in range(n_epochs):\n",
        "        # In every epoch, do training first ...\n",
        "        loss_train = 0.\n",
        "        acc_train = 0.\n",
        "\n",
        "        print(f\"Starting training epoch {i+1}/{n_epochs} ...\")\n",
        "        for captcha_images_batch, texts_class_indices_batch in tqdm(loader_train):\n",
        "            ## [ TODO ]\n",
        "            # Complete the training inner loop:\n",
        "            #\n",
        "            # 1) Move images and labels to the target device\n",
        "            # 2) Perform forward pass to get logits\n",
        "            # 3) Compute loss using `multi_char_loss` and accuracy using `multi_char_acc`\n",
        "            # 4) Perform backward propagation\n",
        "            # 5) Update model parameters by taking an optimizer step\n",
        "            # 6) Clear model gradients\n",
        "            # 7) Accumulate loss and accuracy\n",
        "            #    (Hint: Detach loss tensor before accumulation)\n",
        "            #    (Hint: Move tensors to CPU for accumulation)\n",
        "            raise NotImplementedError\n",
        "\n",
        "        loss_train = (loss_train/len(loader_train)).item()\n",
        "        acc_train = (acc_train/len(loader_train)).item()\n",
        "        # Report loss and metrics\n",
        "        print(f\"Ending training of epoch {i+1}/{n_epochs}: loss: {loss_train}, accuracy: {acc_train}\")\n",
        "\n",
        "        # Then evaluate the model\n",
        "        loss_vali = 0.\n",
        "        acc_vali = 0.\n",
        "\n",
        "        # Put model in validation mode\n",
        "        model.eval()\n",
        "\n",
        "        print(f\"Starting validation epoch {i+1}/{n_epochs} ...\")\n",
        "        for captcha_images_batch, texts_class_indices_batch in tqdm(loader_vali):\n",
        "            ## [ TODO ]\n",
        "            # Complete the validation inner loop:\n",
        "            #\n",
        "            # 1) Move images and labels to the target device\n",
        "            # 2) Perform forward pass to get logits\n",
        "            # 3) Compute loss using `multi_char_loss` and accuracy using `multi_char_acc`\n",
        "            # 4) Accumulate loss and accuracy (move tensors to CPU for accumulation)\n",
        "            #    (Hint: Move tensors to CPU for accumulation)\n",
        "            raise NotImplementedError\n",
        "\n",
        "        loss_vali = (loss_vali/len(loader_vali)).item()\n",
        "        acc_vali = (acc_vali/len(loader_vali)).item()\n",
        "        # Report loss and metrics\n",
        "        print(f\"Ending validation of epoch {i+1}/{n_epochs}: loss: {loss_vali}, accuracy: {acc_vali}\")\n",
        "\n",
        "        # Save epoch metrics\n",
        "        history.append({\n",
        "            \"epoch\": i+1,\n",
        "            \"loss_train\": loss_train,\n",
        "            \"acc_train\": acc_train,\n",
        "            \"loss_vali\": loss_vali,\n",
        "            \"acc_vali\": acc_vali\n",
        "        })\n",
        "\n",
        "        # Put model back in training mode\n",
        "        model.train()\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIiEZ3hBaEXz"
      },
      "source": [
        "Ok, it's time to put everything together. We will build our model, train our model for some epochs and save it as `model-basic-params.pt`. For model training, you may adjust the number of epochs and / or batch size if necessary, to ensure that you achieve at least 80% of accuracy on the training set and at least 70% of accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6mwGVcGDaEXz"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Detect best PyTorch device\n",
        "# (Prefer GPU devices such as CUDA or MPS, then fall back to CPU as last resort)\n",
        "torch_device = hw2_util.get_torch_device()\n",
        "\n",
        "# Build training and validation datasets\n",
        "dataset_train = CAPTCHADataset(captcha_images_train, texts_class_indices_train, captcha_transform)\n",
        "dataset_vali = CAPTCHADataset(captcha_images_vali, texts_class_indices_vali, captcha_transform)\n",
        "\n",
        "# Build a basic end-to-end CAPTCHA model\n",
        "model_basic = build_model(n_classes)\n",
        "# Move model to PyTorch device\n",
        "model_basic = model_basic.to(torch_device)\n",
        "# Build an AdamW optimizer with model parameters\n",
        "optimizer = AdamW(model_basic.parameters(), lr=0.001)\n",
        "\n",
        "# Show model structure\n",
        "print(\"Model structure:\")\n",
        "print(model_basic)\n",
        "print()\n",
        "\n",
        "# Train the basic model for 100 epochs\n",
        "history_basic = train_model(model_basic, optimizer, dataset_train, dataset_vali)\n",
        "# Save the basic model in file\n",
        "torch.save(model_basic.state_dict(), \"./model-basic-params.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPIsonGMaEXz"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We have obtained a trained model as well as the training history during the previous stage. The training history contains the loss and accuracy of the nerual network as the training process proceeded. To show their trend during training, we plot these metrics for both training and validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7zHdQQ9aEXz"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "## [ TODO ]\n",
        "# 1) Plot the loss of the model during training for both training set and validation set.\n",
        "#    (Hint: for history list of dicts, training loss is `history[epoch][\"loss_train\"]` and `history[epoch][\"loss_vali\"]`)\n",
        "# 2) Plot the accuracy of the model during training for both training set and validation set.\n",
        "#    (Hint: for history list of dicts, training accuracy is `history[epoch][\"acc_train\"]` and `history[epoch][\"acc_vali\"]`)\n",
        "#    (Note: plot all curves for the same metric on the same graph for comparison)\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YIrmmI8aEXz"
      },
      "source": [
        "Next, we evaluate our model end-to-end on the test set. Like training and validation set, we transform test set CAPTCHA images through data augmentation. We then predict the characters for these images and compare the predictions with ground truth. Similar to lab 3, we compute the accuracy of our model and show samples of correct and incorrect predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cJMKgQraEXz"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, le, dataset_eval, batch_size=32, n_epochs=5, n_display_samples=10, device=None):\n",
        "    # Detect device from model parameters if not provided\n",
        "    device = device or next(iter(model.parameters())).device\n",
        "\n",
        "    ## [ TODO ]\n",
        "    # Create evaluation data loader from `dataset_eval`\n",
        "    # (Note: set the batch size)\n",
        "    loader_eval = NotImplemented\n",
        "\n",
        "    # Number of test samples and correct predictions\n",
        "    n_test = n_correct = 0\n",
        "    # Correct and incorrect samples\n",
        "    correct_samples = []\n",
        "    incorrect_samples = []\n",
        "\n",
        "    # Put model in validation mode\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(n_epochs):\n",
        "        # In every epoch, do training first ...\n",
        "        print(f\"Starting evaluation epoch {i+1}/{n_epochs} ...\")\n",
        "        for images_batch, texts_batch in tqdm(loader_eval):\n",
        "            # Make a copy of images batch on target PyTorch device, while keeping original batch on CPU\n",
        "            images_batch_device = images_batch.to(device)\n",
        "\n",
        "            ## [ TODO ]\n",
        "            # 1) Predict characters with neural network model\n",
        "            # 2) Flatten and move predictions to CPU\n",
        "            # 3) Decode predicted class indices back to characters using `LabelEncoder.inverse_transform`\n",
        "            # 4) Group every 4 characters together to form predicted CAPTCHAs\n",
        "            #    (Hint: `hw2_util.group_every` can help with grouping)\n",
        "            # 5) For each CAPTCHA, its prediction and actual text:\n",
        "            #    - Update number of correct predictions\n",
        "            #    - Collect `n_display_samples` correct samples of tuple (image, prediction) for review\n",
        "            #    - Collect `n_display_samples` incorrect samples of tuple (image, prediction, actual) for review\n",
        "            #      (Caution: DO NOT print ALL correct / incorrect images; this is slow and the output will be messy!)\n",
        "            raise NotImplementedError\n",
        "\n",
        "        # Report loss and metrics\n",
        "        print(f\"Ending evaluation of epoch {i+1}/{n_epochs} ...\")\n",
        "\n",
        "    # Put model back in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Show statistics\n",
        "    print()\n",
        "    print(\"# of test CAPTCHAs:\", n_test)\n",
        "    print(\"# correctly recognized:\", n_correct)\n",
        "    print(\"Accuracy:\", n_correct/n_test, \"\\n\")\n",
        "\n",
        "    assert len(correct_samples)<=n_display_samples, \\\n",
        "        f\"you should only display {n_display_samples} correct samples as examples!\"\n",
        "    assert len(incorrect_samples)<=n_display_samples, \\\n",
        "        f\"you should only display {n_display_samples} incorrect samples as examples!\"\n",
        "\n",
        "    # Show all correct predictions\n",
        "    hw2_util.print_images(\n",
        "        hw2_util.unzip(correct_samples)[0],\n",
        "        texts=[f\"Correct: {actual}\" for _, actual in correct_samples],\n",
        "        n_rows=2\n",
        "    )\n",
        "\n",
        "    # Show all incorrect predictions\n",
        "    hw2_util.print_images(\n",
        "        hw2_util.unzip(incorrect_samples)[0],\n",
        "        texts=[\n",
        "            f\"Prediction: {pred}\\nActual: {actual}\" \\\n",
        "            for _, pred, actual in incorrect_samples\n",
        "        ],\n",
        "        n_rows=2,\n",
        "        fig_size=(20, 6),\n",
        "        text_center=(0.5, -0.25)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ahjJsuHaEXz"
      },
      "source": [
        "Now we run the evaluation process on the basic model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lnmBLnZRaEXz"
      },
      "outputs": [],
      "source": [
        "# Build evaluation dataset\n",
        "dataset_eval = CAPTCHADataset(captcha_images_test, captcha_texts_test, captcha_transform)\n",
        "\n",
        "# Evaluate the basic model\n",
        "evaluate_model(model_basic, le, dataset_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxG4BaGaEX0"
      },
      "source": [
        "## Questions\n",
        "\n",
        "1. What is the purpose and benefits of data augmentation?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAaeTijfaEX0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VllqyrFaaEX0"
      },
      "source": [
        "2. What is the purpose of 1\\*1 convolution kernel?\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqaFqS3CaEX0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuwWA4pFaEX0"
      },
      "source": [
        "3. What is the purpose of dropout? Does it work on improving the convergence of training or on generalization? Give some reasons to support your idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5FrTTs5aEX0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPHStUbIaEX0"
      },
      "source": [
        "4. What is the purpose of batch normalization? Does it work on improving the convergence of training or on generalization? Give some reasons to support your idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr0HbypbaEX0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iORzXBlsaEXz"
      },
      "source": [
        "## Improvements (Optional; Not Graded)\n",
        "\n",
        "Like homework 1, **you can optionally choose to work on two improvements to the neural network model**, and see if these modifications help with the accuracy. Please note that we won't grade your solutions for these questions, but we may give you feedbacks should you choose to complete them. We also encourage you to **reuse previous code and functions** as much as possible, and avoid repeating identicial code from previous part of this homework.\n",
        "\n",
        "***\n",
        "\n",
        "   Neural network models are susceptible to over-fitting problems due to their huge amount of free parameters. When neural network model overfits, it performs extremely well on the training set, but can easily fail when new samples are added. There are three possible approaches to mitigate the over-fitting issue:\n",
        "\n",
        "  * Adding regularization (L1 and/or L2) to parameters of each linear or convolution layer.\n",
        "  * Adding dropout layers (`torch.nn.Dropout`) to the model.\n",
        "  * Adding batch normalization layers (`torch.nn.BatchNorm2d`) to the model.\n",
        "  \n",
        "Try one or a few of these approaches and compare their performance with the original model. Complete the following tasks by adding code and text cells  above and compare their performance with the original model. Complete the following tasks by adding code and text cells below:\n",
        "  \n",
        "  * Specify your choice of approaches and the details for each approach:\n",
        "    - For regularization, this is the type of regularization and regularization factor for each convolution and linear layer.\n",
        "    - For dropout layers, specify the number, position and dropout rate of all dropout layers.\n",
        "    - For batch normalization layers, specify the number and position of all batch normalization layers.\n",
        "  * Plot the loss and accuracy for both training and validation set. Think about what curves to put on the same plot.\n",
        "    - One idea is to put the same metric of all four models (basic model and three improvement models) for the same set (e.g. training set) on the same plot. In this case, you will get four plots in the end.\n",
        "    - You don't need to include plots as images in the notebook. Instead, just provide the code that can plot these curves.\n",
        "  * Finally, report the accuracy of all three improvement models on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9wwPjs5aEX0"
      },
      "source": [
        "## Submission\n",
        "\n",
        "When youâ€™re done, push this completed notebook (including all code and discussion answers) to your GitHub repo, then submit the repo link on the Canvas submission page.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE-hquc6aEX4"
      },
      "source": [
        "## References\n",
        "\n",
        "1. PyTorch API reference: https://pytorch.org/docs/stable/index.html\n",
        "2. PyTorch Neural Network Modules: https://pytorch.org/docs/stable/nn.html\n",
        "3. Over-fitting: https://en.wikipedia.org/wiki/Overfitting\n",
        "4. Residual neural network: https://en.wikipedia.org/wiki/Residual_neural_network\n",
        "5. Residual blocks - Building blocks of ResNet: https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}